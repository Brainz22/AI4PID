{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de9e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 00:02:23.712082: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Generic Libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os,re\n",
    "import random\n",
    "from PIL import Image \n",
    "\n",
    "# TensorFlow / Keras Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "#Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SKLearn Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3bf9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e42c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"Dataset/train\"\n",
    "test_images = \"Dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls Dataset/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls newFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12dcf3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 936 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 00:02:33.479625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.493212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.493462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.493882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-04 00:02:33.494492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.494697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.494879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.744874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.745059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.745202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-04 00:02:33.745322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14 MB memory:  -> device: 0, name: NVIDIA GeForce GT 1030, pci bus id: 0000:0a:00.0, compute capability: 6.1\n",
      "2023-07-04 00:02:33.755431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 14.00M (14680064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-07-04 00:02:33.755512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 12.60M (13212160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-07-04 00:02:33.755583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 11.34M (11890944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-07-04 00:02:33.755651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 10.21M (10702080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-07-04 00:02:33.755720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 9.19M (9632000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-07-04 00:02:33.755789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 8.27M (8668928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "image_size = (180, 180)\n",
    "batch_size = 10\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_images,\n",
    "    #validation_split=0,\n",
    "    #subset=\"both\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35628e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=2)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (1, 1), activation='relu', input_shape=(180,180,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(1,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(1,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(96, kernel_size=(1,1), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(32, kernel_size=(1,1), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3a1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68adb706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 00:03:52.169799: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 147.63MiB (rounded to 154800128)requested by op StatelessRandomUniformV2\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-04 00:03:52.169848: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-07-04 00:03:52.169857: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 20, Chunks in use: 20. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 517B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169862: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169866: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169870: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 2, Chunks in use: 1. 6.0KiB allocated for chunks. 2.5KiB in use in bin. 2.3KiB client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169873: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169877: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169880: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169884: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 39.2KiB allocated for chunks. 39.2KiB in use in bin. 39.1KiB client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169888: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 1, Chunks in use: 0. 78.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169891: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169894: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169898: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169901: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169904: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169908: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 7.31MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169911: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169914: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169918: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169921: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169924: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169927: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-04 00:03:52.169938: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 147.63MiB was 128.00MiB, Chunk State: \n",
      "2023-07-04 00:03:52.169942: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 7802112\n",
      "2023-07-04 00:03:52.169948: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00000 of size 1280 next 1\n",
      "2023-07-04 00:03:52.169952: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00500 of size 256 next 2\n",
      "2023-07-04 00:03:52.169955: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00600 of size 256 next 3\n",
      "2023-07-04 00:03:52.169958: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00700 of size 256 next 4\n",
      "2023-07-04 00:03:52.169960: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00800 of size 256 next 5\n",
      "2023-07-04 00:03:52.169963: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00900 of size 256 next 6\n",
      "2023-07-04 00:03:52.169966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00a00 of size 256 next 7\n",
      "2023-07-04 00:03:52.169968: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00b00 of size 256 next 8\n",
      "2023-07-04 00:03:52.169971: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00c00 of size 256 next 9\n",
      "2023-07-04 00:03:52.169974: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00d00 of size 256 next 10\n",
      "2023-07-04 00:03:52.169976: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00e00 of size 256 next 11\n",
      "2023-07-04 00:03:52.169979: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e00f00 of size 256 next 12\n",
      "2023-07-04 00:03:52.169982: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01000 of size 256 next 14\n",
      "2023-07-04 00:03:52.169984: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01100 of size 256 next 15\n",
      "2023-07-04 00:03:52.169987: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01200 of size 256 next 13\n",
      "2023-07-04 00:03:52.169989: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01300 of size 256 next 16\n",
      "2023-07-04 00:03:52.169992: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01400 of size 256 next 21\n",
      "2023-07-04 00:03:52.169995: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01500 of size 256 next 19\n",
      "2023-07-04 00:03:52.169997: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01600 of size 256 next 20\n",
      "2023-07-04 00:03:52.170000: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01700 of size 256 next 24\n",
      "2023-07-04 00:03:52.170003: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e01800 of size 256 next 25\n",
      "2023-07-04 00:03:52.170005: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2106e01900 of size 3584 next 17\n",
      "2023-07-04 00:03:52.170008: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e02700 of size 2560 next 18\n",
      "2023-07-04 00:03:52.170011: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2106e03100 of size 80384 next 23\n",
      "2023-07-04 00:03:52.170013: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f2106e16b00 of size 40192 next 22\n",
      "2023-07-04 00:03:52.170017: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f2106e20800 of size 7668992 next 18446744073709551615\n",
      "2023-07-04 00:03:52.170020: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-07-04 00:03:52.170025: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 20 Chunks of size 256 totalling 5.0KiB\n",
      "2023-07-04 00:03:52.170029: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-07-04 00:03:52.170032: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2560 totalling 2.5KiB\n",
      "2023-07-04 00:03:52.170035: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 40192 totalling 39.2KiB\n",
      "2023-07-04 00:03:52.170039: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 48.0KiB\n",
      "2023-07-04 00:03:52.170042: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 7802112 memory_limit_: 14680064 available bytes: 6877952 curr_region_allocation_bytes_: 29360128\n",
      "2023-07-04 00:03:52.170048: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                        14680064\n",
      "InUse:                           49152\n",
      "MaxInUse:                       128768\n",
      "NumAllocs:                          36\n",
      "MaxAllocSize:                    40192\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-07-04 00:03:52.170052: W tensorflow/tsl/framework/bfc_allocator.cc:492] **__________________________________________________________________________________________________\n",
      "2023-07-04 00:03:52.170073: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at stateless_random_ops_v2.cc:67 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[774000,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[774000,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#x = GlobalAveragePooling1D()(x)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(x)\n\u001b[0;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n",
      "File \u001b[0;32m~/micromamba/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/micromamba/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[1;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[1;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2113\u001b[0m )\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[774000,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]"
     ]
    }
   ],
   "source": [
    "# Compile the network\n",
    "x = inputs = Input(shape=(180, 180,3))\n",
    "x = Conv1D(\n",
    "    filters=50,\n",
    "    kernel_size=4,\n",
    "    strides=2,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = Conv1D(filters=50, kernel_size=4,strides=1, activation=\"relu\")(x)\n",
    "#x = GlobalAveragePooling1D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dense(10, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66470fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61249a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37224798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
